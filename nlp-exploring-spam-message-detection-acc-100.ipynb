{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"Welcome to our spam message detection project! Here, we'll leverage machine learning to differentiate between legitimate messages and spam. We'll employ AI to explore prediction using different machine learning models. Let's begin this practical exploration!","metadata":{}},{"cell_type":"markdown","source":"If you are not familiar with text cleaning data here are the steps that we are going to follow :\n\n* LowerCase\n* Tokenize\n* Remove punctuation\n* Remove stopwords\n* Lemmatizing\n\nAfter We are going to Vectorize our data into a set of numbers to make it possible for the Machine Learning Models to learn from it because ML models only proccess numerical features :\n\n* Vectorizing (TfidfVectorizer)","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport nltk\nimport string\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as ltb\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score ","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-08-26T01:38:04.714311Z","iopub.execute_input":"2023-08-26T01:38:04.714754Z","iopub.status.idle":"2023-08-26T01:38:04.724045Z","shell.execute_reply.started":"2023-08-26T01:38:04.714718Z","shell.execute_reply":"2023-08-26T01:38:04.722505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:04.727098Z","iopub.execute_input":"2023-08-26T01:38:04.727516Z","iopub.status.idle":"2023-08-26T01:38:04.764603Z","shell.execute_reply.started":"2023-08-26T01:38:04.72748Z","shell.execute_reply":"2023-08-26T01:38:04.763413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:04.766958Z","iopub.execute_input":"2023-08-26T01:38:04.767842Z","iopub.status.idle":"2023-08-26T01:38:04.784922Z","shell.execute_reply.started":"2023-08-26T01:38:04.767794Z","shell.execute_reply":"2023-08-26T01:38:04.783925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a simple dataset with 5572 observations with **no Missing Values** .We also got one feature **(Message)** and one target **(Category)**","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"Category\",data=data, palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:04.786084Z","iopub.execute_input":"2023-08-26T01:38:04.787111Z","iopub.status.idle":"2023-08-26T01:38:05.026636Z","shell.execute_reply.started":"2023-08-26T01:38:04.787074Z","shell.execute_reply":"2023-08-26T01:38:05.02533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that most of our messages are ham , it's true that this means our dataset is inbalanced but it makes sens because most of the messages in general are **ham messages** However, we are going later to balance this dataset","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Text data preprocessing stands as a vital preliminary phase in natural language processing (NLP). Its essence lies in eliminating disturbances, irregularities, and extraneous content from the text, thereby enhancing data integrity for subsequent analysis. Our approach aligns with the outlined steps in the Introduction.","metadata":{}},{"cell_type":"markdown","source":"### Removing Punctuation and Stopwords","metadata":{}},{"cell_type":"code","source":"def cleaning (text):\n    text = text.lower()\n    text =  re.sub(r'@\\S+', '',text)\n    text =  re.sub(r'http\\S+', '',text) # remove urls\n    text =  re.sub(r'pic.\\S+', '',text)\n    text =  re.sub(r\"[^a-zA-ZáéíóúÁÉÍÓÚ']\", ' ',text) # only keeps characters\n    text =  re.sub(r'\\s+[a-zA-ZáéíóúÁÉÍÓÚ]\\s+', ' ', text+' ')  # keep words with length>1 only\n    text = \"\".join([i for i in text if i not in string.punctuation])\n    words = word_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')   # remove stopwords\n    text = \" \".join([i for i in words if i not in stopwords])\n    text= re.sub(\"\\s[\\s]+\", \" \",text).strip()\n    text= re.sub(\"\\s[\\s]+\", \" \",text).strip() # remove repeated/leading/trailing spaces\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:05.030843Z","iopub.execute_input":"2023-08-26T01:38:05.03121Z","iopub.status.idle":"2023-08-26T01:38:05.040728Z","shell.execute_reply.started":"2023-08-26T01:38:05.031176Z","shell.execute_reply":"2023-08-26T01:38:05.03932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"Message\"]=data[\"Message\"].apply(cleaning)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:05.042396Z","iopub.execute_input":"2023-08-26T01:38:05.042861Z","iopub.status.idle":"2023-08-26T01:38:07.605154Z","shell.execute_reply.started":"2023-08-26T01:38:05.042816Z","shell.execute_reply":"2023-08-26T01:38:07.604046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lemmatization","metadata":{}},{"cell_type":"markdown","source":"Lemmatization serves as a text preprocessing method within the realm of natural language processing (NLP) models. Its purpose is to deconstruct words to their fundamental root, facilitating the recognition of resemblances and similarities.","metadata":{}},{"cell_type":"code","source":"def lemm(data):\n    wordnet = WordNetLemmatizer()\n    lemmanized = []\n    for i in range(len(data)):\n        lemmed = []\n        words = word_tokenize(data['Message'].iloc[i])\n        for w in words:\n            lemmed.append(wordnet.lemmatize(w))\n        lemmanized.append(lemmed)\n\n    data['lemmanized'] = lemmanized\n    data['text'] = data['lemmanized'].apply(' '.join)\n    data=data.drop(\"lemmanized\",axis=1)\n    data=data.drop(\"Message\",axis=1)\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:07.606756Z","iopub.execute_input":"2023-08-26T01:38:07.607204Z","iopub.status.idle":"2023-08-26T01:38:07.615368Z","shell.execute_reply.started":"2023-08-26T01:38:07.607161Z","shell.execute_reply":"2023-08-26T01:38:07.6142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = lemm(data)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:07.617332Z","iopub.execute_input":"2023-08-26T01:38:07.617807Z","iopub.status.idle":"2023-08-26T01:38:09.070569Z","shell.execute_reply.started":"2023-08-26T01:38:07.617765Z","shell.execute_reply":"2023-08-26T01:38:09.069537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding","metadata":{}},{"cell_type":"markdown","source":"Sure we are not going to leave the target **Category** as an object type because we can't feed it into machine learning models for that we will use a map to encode it's values","metadata":{}},{"cell_type":"code","source":"mesg = {\"ham\":0,\"spam\":1}\ndata[\"Category\"]=data[\"Category\"].map(mesg)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:09.071907Z","iopub.execute_input":"2023-08-26T01:38:09.072228Z","iopub.status.idle":"2023-08-26T01:38:09.080555Z","shell.execute_reply.started":"2023-08-26T01:38:09.072199Z","shell.execute_reply":"2023-08-26T01:38:09.079253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spliting Data","metadata":{}},{"cell_type":"markdown","source":"### OverSampling","metadata":{}},{"cell_type":"markdown","source":"We are going to balance our dataset with a simple methode : we are going to create some samples for the smaller class to make it the same size as the ham class . This methode is called **Oversampling** ","metadata":{}},{"cell_type":"code","source":"clas0,clas1=data[\"Category\"].value_counts()\ndf0=data[data[\"Category\"]==0]\ndf1=data[data[\"Category\"]==1]\ndf1=df1.sample(clas0,replace=True)\ndata = pd.concat([df0,df1])","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:09.084309Z","iopub.execute_input":"2023-08-26T01:38:09.084715Z","iopub.status.idle":"2023-08-26T01:38:09.099401Z","shell.execute_reply.started":"2023-08-26T01:38:09.084666Z","shell.execute_reply":"2023-08-26T01:38:09.098156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = data[\"Category\"]\ndata = data[\"text\"]\nlabels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:09.100584Z","iopub.execute_input":"2023-08-26T01:38:09.101015Z","iopub.status.idle":"2023-08-26T01:38:09.110868Z","shell.execute_reply.started":"2023-08-26T01:38:09.100981Z","shell.execute_reply":"2023-08-26T01:38:09.109878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should now split our data now into training and test sets so we can fit and evaluate our model properly","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:09.112236Z","iopub.execute_input":"2023-08-26T01:38:09.112573Z","iopub.status.idle":"2023-08-26T01:38:09.12385Z","shell.execute_reply.started":"2023-08-26T01:38:09.11253Z","shell.execute_reply":"2023-08-26T01:38:09.122648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"### TF-ID Vectoriser","metadata":{}},{"cell_type":"markdown","source":"Vectorization with TF-IDF (Term Frequency-Inverse Document Frequency) is a method in NLP that converts text into numerical form and it tends to have a good performance in spam detection because it takes into account the importance of words based on their frequency in the document and their rarity across the entire corpus. This can help in distinguishing relevant words from common ones that might not contribute much to the task, potentially leading to better separation between legitimate messages and spam.","metadata":{}},{"cell_type":"code","source":"tfid = TfidfVectorizer()\nX_train = tfid.fit_transform(X_train)\nX_test = tfid.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:09.125106Z","iopub.execute_input":"2023-08-26T01:38:09.126046Z","iopub.status.idle":"2023-08-26T01:38:09.57922Z","shell.execute_reply.started":"2023-08-26T01:38:09.126009Z","shell.execute_reply":"2023-08-26T01:38:09.578015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training models","metadata":{}},{"cell_type":"markdown","source":"### LogisticRegression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\npreds = lr.predict(X_test)\nprint(accuracy_score(preds,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:09.58077Z","iopub.execute_input":"2023-08-26T01:38:09.581122Z","iopub.status.idle":"2023-08-26T01:38:09.677638Z","shell.execute_reply.started":"2023-08-26T01:38:09.581092Z","shell.execute_reply":"2023-08-26T01:38:09.676693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"tree = DecisionTreeClassifier()\ntree.fit(X_train,y_train)\npreds = tree.predict(X_test)\nprint(accuracy_score(preds,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:09.679054Z","iopub.execute_input":"2023-08-26T01:38:09.679425Z","iopub.status.idle":"2023-08-26T01:38:10.014399Z","shell.execute_reply.started":"2023-08-26T01:38:09.679392Z","shell.execute_reply":"2023-08-26T01:38:10.013647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"forest = RandomForestClassifier()\nforest.fit(X_train,y_train)\npreds = forest.predict(X_test)\nprint(accuracy_score(preds,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:10.01565Z","iopub.execute_input":"2023-08-26T01:38:10.016917Z","iopub.status.idle":"2023-08-26T01:38:12.569479Z","shell.execute_reply.started":"2023-08-26T01:38:10.01684Z","shell.execute_reply":"2023-08-26T01:38:12.568675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM","metadata":{}},{"cell_type":"code","source":"clf = ltb.LGBMClassifier()\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\nprint(accuracy_score(preds,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:12.570855Z","iopub.execute_input":"2023-08-26T01:38:12.571398Z","iopub.status.idle":"2023-08-26T01:38:13.882599Z","shell.execute_reply.started":"2023-08-26T01:38:12.571359Z","shell.execute_reply":"2023-08-26T01:38:13.881416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"Notice that we are going to convert X_train and X_test to arrays because Naive Bayes don't support sparse data created by the TFidVectoriser","metadata":{}},{"cell_type":"code","source":"naive = GaussianNB()\nnaive.fit(X_train.toarray(), y_train)\npreds = naive.predict(X_test.toarray())\nprint(accuracy_score(preds,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:13.884025Z","iopub.execute_input":"2023-08-26T01:38:13.884451Z","iopub.status.idle":"2023-08-26T01:38:15.234562Z","shell.execute_reply.started":"2023-08-26T01:38:13.884408Z","shell.execute_reply":"2023-08-26T01:38:15.233229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Machine","metadata":{}},{"cell_type":"code","source":"SVM = SVC()\nSVM.fit(X_train, y_train)\npreds = SVM.predict(X_test)\nprint(accuracy_score(preds,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T01:38:15.236069Z","iopub.execute_input":"2023-08-26T01:38:15.236417Z","iopub.status.idle":"2023-08-26T01:38:18.885256Z","shell.execute_reply.started":"2023-08-26T01:38:15.236387Z","shell.execute_reply":"2023-08-26T01:38:18.884068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! We got a 100% accuracy with Random Forest . Finally i hope you enjoyed this notebook , if so feel free to leave an upvote","metadata":{}}]}